{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic NLP Hands-On Exercise (Using NLTK)\n",
        "\n",
        "This Jupyter Notebook provides a simple, hands-on exercise to practice basic Natural Language Processing (NLP) tasks using the NLTK library. The exercises focus on fundamental NLP techniques like tokenization, stemming, lemmatization, POS tagging, and feature extraction. Follow the instructions carefully and complete the tasks.\n",
        "\n",
        "**Note**: Ensure you have the required libraries installed (`nltk`, `sklearn`). Run the setup cell below to install dependencies and download necessary resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ksab2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\ksab2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\ksab2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\ksab2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ksab2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ksab2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setup: Install and import required libraries\n",
        "#!pip install nltk sklearn\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet as wn\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Tokenization and Normalization\n",
        "\n",
        "**Task**: Split text into words and clean it.\n",
        "\n",
        "**Instructions**:\n",
        "1. Tokenize the text into words.\n",
        "2. Convert to lowercase and remove punctuation.\n",
        "3. Remove stop words (common words like 'the', 'is').\n",
        "4. Print the results for each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized words: ['I', 'love', 'running', 'and', 'jumping', '!', 'It', \"'s\", 'so', 'much', 'FUN', '.']\n",
            "Lowercase: i love running and jumping! it's so much fun.\n",
            "Filtered text: ['love', 'running', 'jumping', '!', \"'s\", 'much', 'fun', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ksab2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ksab2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "text = \"I love running and jumping! It's so much FUN.\"\n",
        "\n",
        "# Step 1: Tokenization\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "print(\"Tokenized words:\", word_tokenize(text))\n",
        "\n",
        "\n",
        "# Step 2: Normalization\n",
        "norm_text = text.lower()  # Lowercase\n",
        "print(\"Lowercase:\", norm_text)\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Stop-word Removal\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_text = [word for word in word_tokenize(norm_text) if word not in stop_words]\n",
        "print(\"Filtered text:\", filtered_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Stemming and Lemmatization\n",
        "\n",
        "**Task**: Reduce words to their base forms.\n",
        "\n",
        "**Instructions**:\n",
        "1. Use the filtered tokens from Exercise 1.\n",
        "2. Apply stemming (chop off word endings).\n",
        "3. Apply lemmatization (find dictionary form).\n",
        "4. Print the stemmed and lemmatized tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemmed tokens: ['love', 'run', 'jump', '!', \"'s\", 'much', 'fun', '.']\n",
            "Lemmatized tokens: ['love', 'run', 'jump', '!', \"'s\", 'much', 'fun', '.']\n"
          ]
        }
      ],
      "source": [
        "# Use filtered tokens from Exercise 1\n",
        "tokens = filtered_text\n",
        "\n",
        "\n",
        "# Step 1: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Stemmed tokens:\", stemmed_tokens)\n",
        "\n",
        "# Step 2: Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token, pos=\"v\") for token in tokens]\n",
        "print(\"Lemmatized tokens:\", lemmatized_tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Part-of-Speech (POS) Tagging\n",
        "\n",
        "**Task**: Label words with their grammatical roles.\n",
        "\n",
        "**Instructions**:\n",
        "1. Tokenize the sentence.\n",
        "2. Apply POS tagging to identify nouns, verbs, etc.\n",
        "3. Print the POS tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized sentences: ['The quick fox runs fast.']\n",
            "POS tags: [('The', 'DT'), ('quick', 'JJ'), ('fox', 'NN'), ('runs', 'VBZ'), ('fast', 'RB'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"The quick fox runs fast.\"\n",
        "\n",
        "# Step 1: Tokenization\n",
        "sentences = sent_tokenize(sentence)\n",
        "print(\"Tokenized sentences:\", sentences)\n",
        "\n",
        "# Step 2: POS Tagging\n",
        "pos_tags = pos_tag(word_tokenize(sentence))\n",
        "print(\"POS tags:\", pos_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Named Entity Recognition\n",
        "\n",
        "**Task**: Identify names of people, places, or organizations.\n",
        "\n",
        "**Instructions**:\n",
        "1. Tokenize the sentence and apply POS tagging.\n",
        "2. Use NLTK's named entity chunker to find entities.\n",
        "3. Print the named entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POS tags: [('Elon', 'NNP'), ('Musk', 'NNP'), ('founded', 'VBD'), ('Tesla', 'NNP'), ('in', 'IN'), ('California', 'NNP'), ('.', '.')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     C:\\Users\\ksab2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Named entities: (S\n",
            "  (PERSON Elon/NNP)\n",
            "  (PERSON Musk/NNP)\n",
            "  founded/VBD\n",
            "  (PERSON Tesla/NNP)\n",
            "  in/IN\n",
            "  (GPE California/NNP)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "text = \"Elon Musk founded Tesla in California.\"\n",
        "\n",
        "# Step 1: Tokenization and POS Tagging\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"POS tags:\", pos_tags)\n",
        "\n",
        "\n",
        "# Step 2: Named Entity Recognition\n",
        "nltk.download('maxent_ne_chunker_tab')  \n",
        "named_entities = ne_chunk(pos_tags)\n",
        "print(\"Named entities:\", named_entities)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: WordNet Exploration\n",
        "\n",
        "**Task**: Find synonyms for a word using WordNet.\n",
        "\n",
        "**Instructions**:\n",
        "1. Use WordNet to find synonyms for the word \"happy\".\n",
        "2. Print the definition and synonyms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore synonyms for 'happy'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submission Instructions\n",
        "\n",
        "1. Ensure all code cells run without errors.\n",
        "2. Save the notebook as `nlp_basic_hands_on_nltk.ipynb`.\n",
        "3. Submit the notebook file for evaluation.\n",
        "\n",
        "**Evaluation Criteria**:\n",
        "- Correctness of code implementation.\n",
        "- Clarity of output for each task.\n",
        "- Proper use of NLTK library."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
